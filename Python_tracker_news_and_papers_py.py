{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freeleafcn/IOS_APP-test1/blob/main/Python_tracker_news_and_papers_py.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "ZKPh_YNfOThe",
        "outputId": "3cd71e81-a393-4aca-eb4d-1a0e8719e3e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'feedparser'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b2387be092fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeep_translator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import requests\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from deep_translator import GoogleTranslator\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Global configurations\n",
        "max_paper_num = 40  # total papers to fetch\n",
        "max_news_num = 30   # total news to fetch\n",
        "paper_limit = 3     # limit of papers per query call\n",
        "news_limit = 5      # limit of news per site\n",
        "\n",
        "# Global configuration for keywords\n",
        "PAPER_KEYWORDS = [\"metamaterial\",\"metasurface\",\"Terahertz\"]\n",
        "NEWS_KEYWORDS = [\"novel\", \"breakthrough\", \"optics\", \"AR\", \"VR\", \"meta\"]\n",
        "\n",
        "# ---------------------------------------\n",
        "# 1) Translation utility\n",
        "# ---------------------------------------\n",
        "def translate_text(text, dest_language):\n",
        "    \"\"\"\n",
        "    Attempts to translate the given text into the specified language (ja or zh-cn).\n",
        "    If translation fails, returns the original text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if dest_language == 'ja':\n",
        "            return GoogleTranslator(source='auto', target='ja').translate(text)\n",
        "        elif dest_language == 'zh-cn':\n",
        "            return GoogleTranslator(source='auto', target='zh-CN').translate(text)\n",
        "        else:\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation failed: {e}\")\n",
        "        return text\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 2) Paper retrieval functions\n",
        "#    (arXiv / Semantic Scholar / Crossref)\n",
        "# ---------------------------------------\n",
        "def fetch_arxiv_papers(keywords, max_results=3):\n",
        "    \"\"\"\n",
        "    Fetches papers from arXiv's API according to the given keywords.\n",
        "    Returns a list of paper dictionaries.\n",
        "    Each call retrieves up to 'max_results' for all given keywords combined.\n",
        "    \"\"\"\n",
        "    papers = []\n",
        "    for keyword in keywords:\n",
        "        query = f\"all:{keyword}\"\n",
        "        arxiv_url = (\n",
        "            f\"http://export.arxiv.org/api/query?\"\n",
        "            f\"search_query={query}&sortBy=submittedDate&sortOrder=descending\"\n",
        "            f\"&max_results={max_results}\"\n",
        "        )\n",
        "        response = requests.get(arxiv_url)\n",
        "        feed = feedparser.parse(response.content)\n",
        "        for entry in feed.entries:\n",
        "            papers.append({\n",
        "                'title': entry.title,\n",
        "                'summary': entry.summary,\n",
        "                'link': entry.link,\n",
        "                'published': entry.published,\n",
        "                'source': 'arXiv',\n",
        "                'language': 'English'\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "\n",
        "def fetch_semantic_scholar_papers(keywords, max_results=3):\n",
        "    \"\"\"\n",
        "    Fetches papers from the Semantic Scholar API according to the given keywords.\n",
        "    Returns a list of paper dictionaries.\n",
        "    Each call retrieves up to 'max_results' for all given keywords combined.\n",
        "    \"\"\"\n",
        "    papers = []\n",
        "    for keyword in keywords:\n",
        "        query = keyword.replace(\" \", \"%20\")\n",
        "        url = (\n",
        "            \"https://api.semanticscholar.org/graph/v1/paper/search?\"\n",
        "            f\"query={query}&limit={max_results}&fields=title,abstract,year,url\"\n",
        "        )\n",
        "        print(\"[SemanticScholar] Requesting:\", url)\n",
        "        response = requests.get(url)\n",
        "        print(\"[SemanticScholar] Status code:\", response.status_code)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            papers_list = data.get('data', [])\n",
        "            print(\"[SemanticScholar] Found items:\", len(papers_list))\n",
        "            for paper in papers_list:\n",
        "                papers.append({\n",
        "                    'title': paper.get('title', ''),\n",
        "                    'summary': paper.get('abstract', ''),\n",
        "                    'link': paper.get('url', ''),\n",
        "                    'published': paper.get('year', ''),\n",
        "                    'source': 'Semantic Scholar',\n",
        "                    'language': 'English'\n",
        "                })\n",
        "        else:\n",
        "            print(f\"[SemanticScholar] Request failed: {response.status_code}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "def fetch_crossref_papers(keywords, max_results=3):\n",
        "    \"\"\"\n",
        "    Fetches papers from Crossref using the open REST API.\n",
        "    Returns a list of paper dictionaries (title, link, etc.).\n",
        "    Each call retrieves up to 'max_results' for all given keywords combined.\n",
        "    \"\"\"\n",
        "    papers = []\n",
        "    base_url = \"https://api.crossref.org/works\"\n",
        "    for keyword in keywords:\n",
        "        params = {\n",
        "            \"query\": keyword,\n",
        "            \"rows\": max_results\n",
        "        }\n",
        "        print(\"[Crossref] Requesting params:\", params)\n",
        "        response = requests.get(base_url, params=params)\n",
        "        print(\"[Crossref] Status code:\", response.status_code)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            items = data.get(\"message\", {}).get(\"items\", [])\n",
        "            print(\"[Crossref] Found items:\", len(items))\n",
        "            for item in items:\n",
        "                title_list = item.get(\"title\", [])\n",
        "                title_str = title_list[0] if title_list else \"\"\n",
        "                summary_str = \"\"  # usually no abstracts from Crossref\n",
        "                published_info = item.get(\"created\", {}).get(\"date-parts\", [[]])\n",
        "                published_str = str(published_info[0]) if published_info else \"\"\n",
        "                link_str = item.get(\"URL\", \"\")\n",
        "                papers.append({\n",
        "                    'title': title_str,\n",
        "                    'summary': summary_str,\n",
        "                    'link': link_str,\n",
        "                    'published': published_str,\n",
        "                    'source': 'Crossref',\n",
        "                    'language': 'English'\n",
        "                })\n",
        "        else:\n",
        "            print(f\"[Crossref] Request failed: {response.status_code}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 3) Evenly fetching papers\n",
        "#    Distribute total among 3 sources & N keywords\n",
        "# ---------------------------------------\n",
        "def fetch_papers_evenly(keywords, total_papers=30):\n",
        "    \"\"\"\n",
        "    Distributes 'total_papers' among all (source, keyword) pairs.\n",
        "    1) We have 3 sources: arXiv, Semantic Scholar, Crossref\n",
        "    2) We have len(keywords) keywords\n",
        "    3) total_slots = 3 * len(keywords)\n",
        "    4) per_slot = total_papers // total_slots\n",
        "    5) remainder = total_papers % total_slots\n",
        "    6) For each (source, keyword) pair, request up to 'per_slot' (+1 if remainder left)\n",
        "    7) Each call also uses the global 'paper_limit' (3) as the maximum per call.\n",
        "\n",
        "    Returns the final combined list, truncated to 'total_papers'.\n",
        "    \"\"\"\n",
        "    sources = [\n",
        "        ('arXiv', fetch_arxiv_papers),\n",
        "        ('SemanticScholar', fetch_semantic_scholar_papers),\n",
        "        ('Crossref', fetch_crossref_papers)\n",
        "    ]\n",
        "\n",
        "    num_sources = len(sources)  # 3\n",
        "    num_keywords = len(keywords)\n",
        "    total_slots = num_sources * num_keywords\n",
        "\n",
        "    if total_slots == 0:\n",
        "        print(\"[fetch_papers_evenly] No sources or keywords.\")\n",
        "        return []\n",
        "\n",
        "    per_slot = total_papers // total_slots\n",
        "    remainder = total_papers % total_slots\n",
        "\n",
        "    results = []\n",
        "    slot_index = 0\n",
        "    for (source_name, func) in sources:\n",
        "        for kw in keywords:\n",
        "            # Basic fair-share limit\n",
        "            limit = per_slot + (1 if slot_index < remainder else 0)\n",
        "            slot_index += 1\n",
        "\n",
        "            # Each call also respects 'paper_limit'\n",
        "            # i.e. we cannot exceed the global paper_limit in a single call\n",
        "            # final_limit is the minimum of 'limit' and 'paper_limit'\n",
        "            final_limit = min(limit, paper_limit)\n",
        "\n",
        "            if final_limit <= 0:\n",
        "                print(f\"[fetch_papers_evenly] Source={source_name}, Keyword='{kw}' -> 0 limit, skipped.\")\n",
        "                continue\n",
        "\n",
        "            # We call the function with a single keyword in a list\n",
        "            # to fetch up to final_limit items\n",
        "            partial = func([kw], max_results=final_limit)\n",
        "            print(f\"[fetch_papers_evenly] Source={source_name}, Keyword='{kw}',\"\n",
        "                  f\" limit={final_limit}, got={len(partial)}\")\n",
        "            results.extend(partial)\n",
        "\n",
        "    # Final truncation\n",
        "    if len(results) > total_papers:\n",
        "        results = results[:total_papers]\n",
        "    print(f\"[fetch_papers_evenly] Combined total after truncation: {len(results)}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 4) News retrieval from websites + even distribution\n",
        "# ---------------------------------------\n",
        "def fetch_news_from_website(website, news_keywords, max_results=5):\n",
        "    \"\"\"\n",
        "    Fetches news items from a single website by parsing the homepage\n",
        "    and matching link text against the given keywords.\n",
        "    \"\"\"\n",
        "    news_items = []\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": (\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                \"Chrome/103.0.0.0 Safari/537.36\"\n",
        "            )\n",
        "        }\n",
        "        response = requests.get(website, timeout=10, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"[News] Failed to fetch {website}, status code: {response.status_code}\")\n",
        "            return news_items\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        all_links = soup.find_all('a')\n",
        "        for link_tag in all_links:\n",
        "            title_text = (link_tag.get_text() or \"\").strip()\n",
        "            if not title_text:\n",
        "                continue\n",
        "            href = link_tag.get('href', '')\n",
        "            if not href.startswith('http'):\n",
        "                from urllib.parse import urljoin\n",
        "                href = urljoin(website, href)\n",
        "\n",
        "            lower_title = title_text.lower()\n",
        "            if any(kw.lower() in lower_title for kw in news_keywords):\n",
        "                news_items.append({\n",
        "                    'title': title_text[:200],\n",
        "                    'summary': \"\",\n",
        "                    'link': href,\n",
        "                    'published': \"\",\n",
        "                    'source': website,\n",
        "                    'language': 'English'\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"[News] Error fetching {website}: {e}\")\n",
        "\n",
        "    return news_items[:max_results]\n",
        "\n",
        "\n",
        "def fetch_news_evenly(websites, news_keywords, total_news=30):\n",
        "    \"\"\"\n",
        "    Distributes 'total_news' across all websites.\n",
        "    If not divisible, some sites get +1.\n",
        "    Also respects the global 'news_limit' (5) for each site.\n",
        "    \"\"\"\n",
        "    num_sites = len(websites)\n",
        "    if num_sites == 0:\n",
        "        print(\"[fetch_news_evenly] No websites provided.\")\n",
        "        return []\n",
        "\n",
        "    per_site = total_news // num_sites\n",
        "    remainder = total_news % num_sites\n",
        "\n",
        "    results = []\n",
        "    for i, site in enumerate(websites):\n",
        "        limit = per_site + (1 if i < remainder else 0)\n",
        "        # Also never exceed the global news_limit\n",
        "        final_limit = min(limit, news_limit)\n",
        "        if final_limit <= 0:\n",
        "            print(f\"[fetch_news_evenly] {site} -> 0 limit, skipped.\")\n",
        "            continue\n",
        "\n",
        "        partial = fetch_news_from_website(site, news_keywords, max_results=final_limit)\n",
        "        print(f\"[fetch_news_evenly] {site} -> requested {final_limit}, got {len(partial)}\")\n",
        "        results.extend(partial)\n",
        "\n",
        "    # Truncate total\n",
        "    results = results[:total_news]\n",
        "\n",
        "    # Remove duplicates by link\n",
        "    unique_news = []\n",
        "    seen_links = set()\n",
        "    for item in results:\n",
        "        if item['link'] not in seen_links:\n",
        "            unique_news.append(item)\n",
        "            seen_links.add(item['link'])\n",
        "\n",
        "    return unique_news\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 5) Generate HTML for both Papers and News\n",
        "# ---------------------------------------\n",
        "def generate_html(papers_df, news_df, filename):\n",
        "    \"\"\"\n",
        "    Generates an HTML file containing both paper and news entries,\n",
        "    each in English, Japanese, and Chinese.\n",
        "    \"\"\"\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Research Papers and News</title>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; }\n",
        "            .item { margin-bottom: 20px; }\n",
        "            .title { font-size: 18px; font-weight: bold; }\n",
        "            .summary { font-size: 14px; color: #555; }\n",
        "            .link { font-size: 14px; color: #007BFF; }\n",
        "            .source { font-size: 12px; color: #888; }\n",
        "            .jump-links { margin-bottom: 20px; }\n",
        "            .jump-links a { margin-right: 10px; }\n",
        "            .language-section { margin-top: 20px; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Research Papers and News</h1>\n",
        "        <div class=\"jump-links\">\n",
        "            <a href=\"#papers\">Jump to Title:Paper</a>\n",
        "            <a href=\"#news\">Jump to Title:News</a>\n",
        "        </div>\n",
        "        <h2 id=\"papers\">Title:Paper</h2>\n",
        "    \"\"\"\n",
        "\n",
        "    # Papers\n",
        "    for _, row in papers_df.iterrows():\n",
        "        title_en = row['title']\n",
        "        summary_en = row['summary']\n",
        "        title_ja = translate_text(title_en, 'ja')\n",
        "        summary_ja = translate_text(summary_en, 'ja')\n",
        "        title_zh = translate_text(title_en, 'zh-cn')\n",
        "        summary_zh = translate_text(summary_en, 'zh-cn')\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"item\">\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">English: {title_en}</div>\n",
        "                <div class=\"summary\">{summary_en}</div>\n",
        "            </div>\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">Japanese: {title_ja}</div>\n",
        "                <div class=\"summary\">{summary_ja}</div>\n",
        "            </div>\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">Chinese: {title_zh}</div>\n",
        "                <div class=\"summary\">{summary_zh}</div>\n",
        "            </div>\n",
        "            <div class=\"link\"><a href=\"{row['link']}\">Read more</a></div>\n",
        "            <div class=\"source\">Source: {row['source']} | Published: {row['published']} | Language: {row['language']}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # News\n",
        "    html_content += \"\"\"\n",
        "        <h2 id=\"news\">Title:News</h2>\n",
        "    \"\"\"\n",
        "    for _, row in news_df.iterrows():\n",
        "        title_en = row['title']\n",
        "        summary_en = row['summary']\n",
        "        title_ja = translate_text(title_en, 'ja')\n",
        "        summary_ja = translate_text(summary_en, 'ja')\n",
        "        title_zh = translate_text(title_en, 'zh-cn')\n",
        "        summary_zh = translate_text(summary_en, 'zh-cn')\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"item\">\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">English: {title_en}</div>\n",
        "                <div class=\"summary\">{summary_en}</div>\n",
        "            </div>\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">Japanese: {title_ja}</div>\n",
        "                <div class=\"summary\">{summary_ja}</div>\n",
        "            </div>\n",
        "            <div class=\"language-section\">\n",
        "                <div class=\"title\">Chinese: {title_zh}</div>\n",
        "                <div class=\"summary\">{summary_zh}</div>\n",
        "            </div>\n",
        "            <div class=\"link\"><a href=\"{row['link']}\">Read more</a></div>\n",
        "            <div class=\"source\">Source: {row['source']} | Published: {row['published']} | Language: {row['language']}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Write to file\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(html_content)\n",
        "    print(f\"{filename} generated successfully.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6) Send email with HTML body + attachment\n",
        "# ---------------------------------------\n",
        "def send_email(subject, body, to_emails, smtp_server, smtp_port,\n",
        "               sender_email, sender_password, attachment_path):\n",
        "    \"\"\"\n",
        "    Sends an email with the specified subject and HTML body,\n",
        "    plus attaches the file at attachment_path.\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = sender_email\n",
        "    msg['To'] = \", \".join(to_emails)\n",
        "    msg['Subject'] = subject\n",
        "\n",
        "    # Attach the HTML body\n",
        "    msg.attach(MIMEText(body, 'html'))\n",
        "\n",
        "    # Attach the file\n",
        "    try:\n",
        "        with open(attachment_path, 'rb') as f:\n",
        "            attachedfile = MIMEBase('application', 'octet-stream')\n",
        "            attachedfile.set_payload(f.read())\n",
        "        encoders.encode_base64(attachedfile)\n",
        "        filename_only = os.path.basename(attachment_path)\n",
        "        attachedfile.add_header('Content-Disposition', f'attachment; filename=\"{filename_only}\"')\n",
        "        msg.attach(attachedfile)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to attach file: {e}\")\n",
        "\n",
        "    # Send the email\n",
        "    try:\n",
        "        with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
        "            server.starttls()\n",
        "            server.login(sender_email, sender_password)\n",
        "            server.sendmail(sender_email, to_emails, msg.as_string())\n",
        "        print(\"Email sent successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to send email: {e}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 7) Main script flow\n",
        "# ---------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # PAPER SETTINGS\n",
        "    TOTAL_PAPERS = max_paper_num\n",
        "    #PAPER_KEYWORDS = [\"metamaterial\",\"metasurface\",\"Terahertz\"]  # Example\n",
        "\n",
        "    # NEWS SETTINGS\n",
        "    TOTAL_NEWS = max_news_num\n",
        "    NEWS_SOURCES = [\n",
        "        \"https://www.sciencenews.org/\",\n",
        "        \"https://www.newscientist.com/\",\n",
        "        \"https://arstechnica.com/\",\n",
        "        \"https://www.sciencedaily.com/\",\n",
        "        \"https://www.nature.com/news\"\n",
        "    ]\n",
        "    #NEWS_KEYWORDS = [\"novel\", \"breakthrough\", \"optics\", \"AR\", \"VR\", \"meta\"]\n",
        "\n",
        "    # 1) Fetch papers (arXiv, Semantic Scholar, Crossref) with new distribution\n",
        "    papers = fetch_papers_evenly(PAPER_KEYWORDS, total_papers=TOTAL_PAPERS)\n",
        "    df_papers = pd.DataFrame(papers)\n",
        "\n",
        "    # 2) Fetch news from websites (distributed among sources)\n",
        "    news = fetch_news_evenly(NEWS_SOURCES, NEWS_KEYWORDS, total_news=TOTAL_NEWS)\n",
        "    df_news = pd.DataFrame(news)\n",
        "\n",
        "    # 3) Generate HTML file\n",
        "    FILENAME = \"papers_and_news.html\"\n",
        "    generate_html(df_papers, df_news, FILENAME)\n",
        "\n",
        "    # 4) Read HTML file for email body\n",
        "    with open(FILENAME, \"r\", encoding=\"utf-8\") as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    # 5) Email configuration\n",
        "    SMTP_SERVER = \"smtp.gmail.com\"\n",
        "    SMTP_PORT = 587\n",
        "    SENDER_EMAIL = \"nitto.otc4@gmail.com\"\n",
        "    SENDER_PASSWORD = \"vrsknojnubccbsma\"\n",
        "    TO_EMAILS = [\"yufeng.weng@nitto.com\"]\n",
        "\n",
        "    # 6) Send email (HTML body + attachment)\n",
        "    send_email(\n",
        "        subject=\"Latest Papers and News\",\n",
        "        body=html_content,\n",
        "        to_emails=TO_EMAILS,\n",
        "        smtp_server=SMTP_SERVER,\n",
        "        smtp_port=SMTP_PORT,\n",
        "        sender_email=SENDER_EMAIL,\n",
        "        sender_password=SENDER_PASSWORD,\n",
        "        attachment_path=FILENAME\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rgs2AtgOThj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}